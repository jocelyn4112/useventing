{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import lxml.html as lh\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import codecs\n",
    "\n",
    "# Turtorial https://towardsdatascience.com/dataset-creation-and-cleaning-web-scraping-using-python-part-2-7dce33cddf66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url='file:///C:/Users/jocel/Documents/GitHub/useventing/Event1.html'\n",
    "# #Create a handle, page, to handle the contents of the website\n",
    "# page = requests.get(url)\n",
    "# #Store the contents of the website under doc\n",
    "# doc = lh.fromstring(page.content)\n",
    "# #Parse data that are stored between <tr>..</tr> of HTML\n",
    "# tr_elements = doc.xpath('//tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-bfff09c81895>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Get table and prettify\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'table'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtable\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'soup' is not defined"
     ]
    }
   ],
   "source": [
    "# Get table and prettify\n",
    "\n",
    "content = soup\n",
    "tables = content.find_all('table')\n",
    "for table in tables:\n",
    "    print(table.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Get data from table \n",
    "# table = soup.find({'class': 'brd-div'})\n",
    "# rows = table.find_all('tr')\n",
    "# print(soup)\n",
    "\n",
    "# res = requests.get('soup')\n",
    "# soup = BeautifulSoup(res.content,'lxml')\n",
    "# table = soup.find_all('table')[0] \n",
    "# df = pd.read_html(str(table))\n",
    "# print(df[0].to_json(orient='records'))\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List of all links\n",
    "# for row in rows:\n",
    "#     cells = row.find_all('td')\n",
    "#     if len(cells) > 1:\n",
    "#         rider_link = cells[1].find('a')\n",
    "#         print(rider_link.get('href'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we can simply add all our knowledge to the function getAdditionalDetails(). We define the variable read_content to flag if we read the next value or not. We use three types of functions here, apart from those described before:\n",
    "# get(): This enables us to not only find but also get the reference to a particular element.\n",
    "# get_text(): This gets the value that is present within the opening and closing tags of an element.\n",
    "# strip(): This removes any additional leading and trailing spaces that might be present in the text. We can also specify any specific value we might wish to be removed, for example, in out case the new line character \\n.\n",
    "\n",
    "# def getAdditionalDetails(url):\n",
    "#     try:\n",
    "#         country_page = getHTMLContent('https://en.wikipedia.org' + url)\n",
    "#         table = country_page.find('table', {'class': 'infobox geography vcard'})\n",
    "#         additional_details = []\n",
    "#         read_content = False\n",
    "#         for tr in table.find_all('tr'):\n",
    "#             if (tr.get('class') == ['mergedtoprow'] and not read_content):\n",
    "#                 link = tr.find('a')\n",
    "#                 if (link and (link.get_text().strip() == 'Area' or\n",
    "#                    (link.get_text().strip() == 'GDP' and tr.find('span').get_text().strip() == '(nominal)'))):\n",
    "#                     read_content = True\n",
    "#                 if (link and (link.get_text().strip() == 'Population')):\n",
    "#                     read_content = False\n",
    "#             elif ((tr.get('class') == ['mergedrow'] or tr.get('class') == ['mergedbottomrow']) and read_content):\n",
    "#                 additional_details.append(tr.find('td').get_text().strip('\\n')) \n",
    "#                 if (tr.find('div').get_text().strip() != '•\\xa0Total area' and\n",
    "#                    tr.find('div').get_text().strip() != '•\\xa0Total'):\n",
    "#                     read_content = False\n",
    "#         return additional_details\n",
    "#     except Exception as error:\n",
    "#         print('Error occured: {}'.format(error))\n",
    "#         return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Create dataset to csv\n",
    "# data_content = []\n",
    "# for row in rows:\n",
    "#     cells = row.find_all('td')\n",
    "#     if len(cells) > 1:\n",
    "#         print(cells[1].get_text())\n",
    "#         country_link = cells[1].find('a')\n",
    "#         country_info = [cell.text.strip('\\n') for cell in cells]\n",
    "#         additional_details = getAdditionalDetails(country_link.get('href'))\n",
    "#         if (len(additional_details) == 4):\n",
    "#             country_info += additional_details\n",
    "#             data_content.append(country_info)\n",
    "\n",
    "# dataset = pd.DataFrame(data_content)\n",
    "\n",
    "# # Define column headings\n",
    "# headers = rows[0].find_all('th')\n",
    "# headers = [header.get_text().strip('\\n') for header in headers]\n",
    "# headers += [\n",
    "#             \"Rider\",\n",
    "#             \"Horse\",\t\n",
    "#              \"Dress Score\",\n",
    "#              \"Dress Place\",\n",
    "#              \"XC Jump Penalties\"\n",
    "#              \"XC Elapsed Time\",\n",
    "#              \"XC Time\",\n",
    "#              \"Two Phase Score\"\n",
    "#              \"Two Phase Place\",\n",
    "\n",
    "#             \"Stadium Penalties\",\n",
    "#             \"Stadium Time\",\n",
    "#             \"Final Score\",\n",
    "#             \" Final Place\"\n",
    "#     ]\n",
    "# dataset.columns = headers\n",
    "\n",
    "# #drop_columns = ['Rank', 'Date', 'Source']\n",
    "# dataset.drop(drop_columns, axis = 1, inplace = True)\n",
    "# dataset.sample(3)\n",
    "\n",
    "# dataset.to_csv(\"Dataset.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-3305119f57c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"https://evententries.com/#LiveScores_pGAtQsPZt0FQ4zn43EMKQ==\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'lxml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'table'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_html\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'records'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#try using staright html \n",
    "\n",
    "res = requests.get(\"https://evententries.com/#LiveScores_pGAtQsPZt0FQ4zn43EMKQ==\")\n",
    "soup = BeautifulSoup(res.content,'lxml')\n",
    "table = soup.find_all('table')[0] \n",
    "df = pd.read_html(str(table))\n",
    "print(df[0].to_json(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(\"http://www.nationmaster.com/country-info/stats/Media/Internet-users\")\n",
    "soup = BeautifulSoup(res.content,'lxml')\n",
    "table = soup.find_all('table')[0] \n",
    "df = pd.read_html(str(table))\n",
    "print(df[0].to_json(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to scrape\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Read in the raw HTML file and load it into BeautifulSoup\n",
    "with open('yourfile.html') as inf:\n",
    "    soup = BeautifulSoup(inf.read(), 'html.parser')\n",
    "\n",
    "# Dump out the HTML in an easier to read format\n",
    "#print(soup.prettify())\n",
    "\n",
    "# Find all results tables. Each table is wrapped by a div with class \"brd-div\"\n",
    "# so we can find all those divs and then access the child table within each div\n",
    "for div in soup.find_all('div', class_=\"brd-div\"):\n",
    "    table = div.find_next('table')\n",
    "    # Now we have the table:\n",
    "    # - the first row is the name of the trial\n",
    "    # - the 2nd row is the \"Last updated\" row\n",
    "    # - the 3rd row is the column headers\n",
    "    # - all remaining rows are the data\n",
    "\n",
    "    # Get the first row\n",
    "    trial_row = table.find_next('tr')\n",
    "    # Find the a tag and retrieve the inner text - this is the trial name string\n",
    "    trial = trial_row.find_next('a').contents[0]\n",
    "\n",
    "    # Get the next row (row 2)\n",
    "    last_updated_row = trial_row.next_sibling\n",
    "    # If you want the last updated time, pull it out here\n",
    "\n",
    "    # Get the next row (row 3)\n",
    "    col_header_row = last_updated_row.next_sibling\n",
    "    # Iterate through the columns (td elements) in the row to extract the\n",
    "    # column headers\n",
    "    col_headers = []\n",
    "    for td in col_header_row.find_all('td'):\n",
    "        # The td elements we want have an inner div element with a class value\n",
    "        # of gwt-HTML, the contents of that div are the label we want and the\n",
    "        # label is split across two lines except the Rider/Horse column which\n",
    "        # we need to treat as 2 separate columns\n",
    "        if len(td.contents) == 3:\n",
    "            # This is the Rider/Horse td\n",
    "            col_headers.append(td.contents[0])\n",
    "            col_headers.append(td.contents[2])\n",
    "        else:\n",
    "            inner_div = td.find('div', class_='gwt-HTML')\n",
    "            if inner_div:\n",
    "                label = inner_div.contents[0] + \" \" + inner_div.contents[2]\n",
    "                if label == 'Rider Horse':\n",
    "                    col_headers += label.split(' ')\n",
    "                else:\n",
    "                    col_headers.append(label)\n",
    "\n",
    "    # Now iterate over the remaining rows in the table which contain the data\n",
    "    # Every row contains a div with class gwt-HTML like the header row except\n",
    "    # the Rider/Horse column which has 2 values separated by a <br/> tag\n",
    "    data_rows = []\n",
    "    for row in col_header_row.next_siblings:\n",
    "        data_row = []\n",
    "        for td in row.find_all('td'):\n",
    "            if len(td.contents) == 3:\n",
    "                # This is the Rider/Horse td\n",
    "                data_row.append(td.contents[0])\n",
    "                # The Horse name is wrapped in <i> tags so we get the contents\n",
    "                # of that inner tag\n",
    "                data_row.append(td.contents[2].contents[0])\n",
    "            else:\n",
    "                inner_div = td.find('div', class_='gwt-HTML')\n",
    "                if inner_div:\n",
    "                    # The content of this div is the data value\n",
    "                    data_row.append(inner_div.contents[0])\n",
    "        data_rows.append(data_row)\n",
    "    df = pd.DataFrame.from_records(data_rows, columns=col_headers)\n",
    "    print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
